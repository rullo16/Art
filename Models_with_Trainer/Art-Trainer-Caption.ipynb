{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Art Trainer Captioning Model\n",
    "\n",
    "This notebook details the process of fine-tuning an image captioning model using a dataset of art descriptions. We use the Git-large-Coco model from Microsoft and employ the HuggingFace trainer for fine-tuning. The notebook includes steps for data preprocessing, model selection, evaluation, and hyperparameter optimization.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Installation](#Installation)\n",
    "2. [Loading the Dataset](#Loading-the-Dataset)\n",
    "3. [Model Selection](#Model-Selection)\n",
    "4. [Data Preprocessing](#Data-Preprocessing)\n",
    "5. [Evaluation](#Evaluation)\n",
    "6. [Fine Tuning](#Fine-Tuning)\n",
    "7. [Hyperparameters Optimization](#Hyperparameters-Optimization)\n",
    "8. [Fine Tuning with Best Parameters](#Fine-Tuning-with-Best-Parameters)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install the needed packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%pip install datasets\n",
    "%pip install transformers\n",
    "%pip install sentencepiece\n",
    "%pip install diffusers --upgrade\n",
    "%pip install invisible_watermark accelerate safetensors\n",
    "%pip install accelerate\n",
    "%pip install jiwer\n",
    "%pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "import torch\n",
    "from transformers import AutoProcessor,TrainingArguments, Trainer, AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "from PIL import Image, ImageFile\n",
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "Image.LOAD_TRUNCATED_IMAGES = True\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "PYTORCH_CUDA_ALLOC_CONF=expandable_segments = True\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset\n",
    "\n",
    "We check if the dataset is already present in the system, by checking the local variables. If it is not present we load it, else we just skip it. This is done to avoid loading the dataset again and again, as it takes time to load the dataset.\n",
    "\n",
    "## Data to Keep\n",
    "Since in this notebook we only Fine-Tune the Captioning model we only keep the descriptions of the paintings, which we scraped from the urls provided in the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../described_dataset_label.csv',sep='\\t',encoding='latin-1')\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "data = data.iloc[:20000]\n",
    "data = data.rename(columns={'FILE':'image','AUTHOR':'author', 'TECHNIQUE':'style','URL':'description'})\n",
    "data = data[['image','description']]\n",
    "data['image'] = [f'.{x}' for x in data['image']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.Dataset.from_pandas(data).cast_column('image', datasets.Image())\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset[53]\n",
    "\n",
    "image = sample['image']\n",
    "height,width = image.size\n",
    "display(image.resize((int(0.3*height),int(0.3*width))))\n",
    "caption = sample['description']\n",
    "print(caption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Model\n",
    "\n",
    "For our task, we opted for the Git-large-Coco model provided by Microsoft. This model is a large-scale language model trained on the Coco dataset, which shares similarities with our dataset as it consists of images paired with textual descriptions. Among the models available on Hugging Face, we found that the Git-large-Coco model consistently yielded superior results for our specific task.\n",
    "\n",
    "In addition to the model itself, we utilized the associated processor. This processor aligns with the one used during the original training of the model. Ensuring consistency between the model and its processor is crucial, as the processor plays a key role in tokenizing input data. Mismatched processors could result in the model being unable to interpret input data correctly. By leveraging the pre-existing processor, we save significant time and effort that would otherwise be required to develop and fine-tune our own processor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_capt= \"microsoft/git-large-coco\"\n",
    "processor_capt = AutoProcessor.from_pretrained(checkpoint_capt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "To facilitate data feeding into the processor and obtain tokenized inputs for the model, we define a function named `capt_transforms`. This function transforms the raw data into a format suitable for the model input. Here’s a breakdown of the process:\n",
    "\n",
    "## Function Description\n",
    "The `capt_transforms` function takes an example from the dataset and performs the following steps:\n",
    "\n",
    "1. **Extract Images and Captions**:\n",
    "   - Extracts images and captions from the example batch.\n",
    "\n",
    "2. **Tokenization**:\n",
    "   - Utilizes the pre-trained processor (`processor_capt`) to tokenize the images and captions.\n",
    "   - Sets the maximum sequence length for padding and truncates sequences if needed.\n",
    "   - Returns a DatasetDict containing tokenized inputs with keys \"input_ids\" and \"labels\".\n",
    "\n",
    "## Input and Output\n",
    "- **Input**: Example batch containing images and captions.\n",
    "- **Output**: Tokenized inputs suitable for model ingestion, comprising input IDs and corresponding labels.\n",
    "\n",
    "## Data Processing Improvements\n",
    "- **Efficient Transformation**: The function efficiently transforms raw data into tokenized inputs using the pre-trained processor, ensuring compatibility with the model's input requirements.\n",
    "- **Padding and Truncation**: Utilizes padding and truncation to handle sequences of varying lengths, enabling uniform input sizes for the model.\n",
    "- **Label Generation**: Generates labels from input IDs, facilitating model training and evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms(example_batch):\n",
    "    images = [x for x in example_batch[\"image\"]]\n",
    "    captions = [x for x in example_batch[\"description\"]]\n",
    "    inputs = processor_capt(images=images, text=captions, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "    inputs.update({\"labels\": inputs[\"input_ids\"]})\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caption\n",
    "capt_dataset = dataset.train_test_split(test_size=0.3)\n",
    "capt_dataset = capt_dataset.with_transform(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_capt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to preprocess the data before evaluation because the trainer function has a problem of memory leaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_logits_for_metrics(logits,labels):\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    return predictions,labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "For evaluating our model’s performance, we employed the ROUGE metric, which is widely used for evaluating image captioning in generative models. The ROUGE score measures the similarity between generated captions and ground truth captions by comparing overlapping n-grams, thereby providing insights into the quality and accuracy of the generated captions.\n",
    "\n",
    "To compute the ROUGE score, we utilized the `load_metric` function from the  `datasets` library, specifically loading the ROUGE metric for evaluation purposes.\n",
    "We then defined a custom function named `capt_compute_metrics`  library, specifically loading the ROUGE metric for evaluation purposes. We then defined a custom function named capt_compute_metrics to compute the ROUGE score for evaluation. This function takes eval_pred as input, which contains logits (model predictions) and labels (ground truth captions).\n",
    "\n",
    "The function performs the following steps:\n",
    "\n",
    "1. Extracts logits (model predictions) and labels (ground truth captions) from the `eval_pred` input.\n",
    "2. Decodes the labels and predictions using the associated processor (`processor_capt`), skipping special tokens to obtain human-readable text.\n",
    "3. Computes the ROUGE score using the decoded predictions and references (ground truth captions).\n",
    "4. Returns a dictionary containing the computed ROUGE score under the key \"rouge_score\".\n",
    "\n",
    "By utilizing the ROUGE score and implementing a custom evaluation function, we gain valuable insights into the quality of our model’s generated captions compared to ground truth captions. This facilitates quantitative assessment and refinement of the model’s performance, ultimately contributing to its effectiveness in generating accurate and relevant captions for images.\n",
    "\n",
    "We need to preprocess the data before evaluation because the trainer function has a problem of memory leaks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load('rouge')\n",
    "def capt_compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    pred_ids = logits[0]\n",
    "    pred_ids = processor_capt.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels = processor_capt.batch_decode(labels, skip_special_tokens=True)\n",
    "    result = rouge.compute(predictions=pred_ids, references=labels, use_aggregator=True)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# clip = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "# processor_clip = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# def clip_compute_metrics(eval_pred):\n",
    "#     references = eval_pred.label_ids\n",
    "#     generated_text = eval_pred.predictions[1]\n",
    "\n",
    "#     references = processor_clip.batch_decode(references, skip_special_tokens=True)\n",
    "#     generated_text = processor_clip.batch_decode(generated_text, skip_special_tokens=True)\n",
    "\n",
    "#     # Calculate the BERTScore\n",
    "#     result = clip.compute(predictions=generated_text, references=references)\n",
    "#     print(result)\n",
    "#     return result[\"f1\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tuning\n",
    "\n",
    "In this notebook, we decide to use the HuggingFace trainer since it is a very easy-to-use and powerful tool to fine-tune models. This saved us time in writing our own custom training loop as the Hugging Face model serves as a comparison for our custom models, which we train in a different notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters Optimization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init(trial):\n",
    "    return AutoModelForCausalLM.from_pretrained(checkpoint_capt).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "capt_training_args = TrainingArguments(\n",
    "    output_dir=\"model_checkpoints/captioning\",\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=5,\n",
    "    warmup_ratio=0.2,\n",
    "    fp16=False,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=2,\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    label_names=[\"labels\"],\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "capt_trainer = Trainer(\n",
    "    # model=model_capt,\n",
    "    model_init=model_init,\n",
    "    args=capt_training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=capt_dataset[\"train\"],\n",
    "    eval_dataset=capt_dataset[\"test\"],\n",
    "    compute_metrics=capt_compute_metrics,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HyperParameter Search\n",
    "\n",
    "def optuna_hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 1e-4, log=True, step=1e-5),\n",
    "        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 1, 4,step=1),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [6,8]),\n",
    "        \"gradient_accumulation_steps\": trial.suggest_int(\"gradient_accumulation_steps\", 1, 4, step = 1),\n",
    "        \"per_device_eval_batch_size\": trial.suggest_categorical(\"per_device_eval_batch_size\", [6,8]),\n",
    "        \"warmup_ratio\": trial.suggest_float(\"warmup_ratio\", 0.1, 0.3, step=0.1),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trials = capt_trainer.hyperparameter_search(n_trials=100,\n",
    "                                                 backend=\"optuna\",\n",
    "                                                 hp_space=optuna_hp_space, \n",
    "                                                 direction=\"maximize\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine Tune with best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_capt = AutoModelForCausalLM.from_pretrained(checkpoint_capt).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "best_hyperparameters = best_trials.hyperparameters\n",
    "\n",
    "capt_training_args = TrainingArguments(\n",
    "    output_dir=\"model_checkpoints/captioning\",\n",
    "    learning_rate=best_hyperparameters[\"learning_rate\"],\n",
    "    num_train_epochs=best_hyperparameters[\"num_train_epochs\"],\n",
    "    fp16=False,\n",
    "    per_device_train_batch_size=best_hyperparameters[\"per_device_train_batch_size\"],\n",
    "    per_device_eval_batch_size=best_hyperparameters[\"per_device_eval_batch_size\"],\n",
    "    gradient_accumulation_steps=best_hyperparameters[\"gradient_accumulation_steps\"],\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    label_names=[\"labels\"],\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "capt_trainer = Trainer(\n",
    "    model=model_capt,\n",
    "    args=capt_training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=capt_dataset[\"train\"],\n",
    "    eval_dataset=capt_dataset[\"test\"],\n",
    "    compute_metrics=capt_compute_metrics,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "# from transformers import EarlyStoppingCallback\n",
    "# capt_training_args = TrainingArguments(\n",
    "#     output_dir=\"model_checkpoints/captioning\",\n",
    "#     learning_rate=1e-5,\n",
    "#     num_train_epochs=5,\n",
    "#     warmup_ratio=0.2,\n",
    "#     fp16=False,\n",
    "#     per_device_train_batch_size=8,\n",
    "#     per_device_eval_batch_size=8,\n",
    "#     gradient_accumulation_steps=2,\n",
    "#     eval_accumulation_steps=1,\n",
    "#     save_total_limit=2,\n",
    "#     evaluation_strategy=\"no\",\n",
    "#     save_strategy=\"no\",\n",
    "#     remove_unused_columns=False,\n",
    "#     push_to_hub=False,\n",
    "#     label_names=[\"description\"],\n",
    "#     load_best_model_at_end=True,\n",
    "# )\n",
    "\n",
    "# capt_trainer = Trainer(\n",
    "#     model=model_capt,\n",
    "#     # model_init=model_init,\n",
    "#     args=capt_training_args,\n",
    "#     data_collator=data_collator,\n",
    "#     train_dataset=capt_dataset[\"train\"],\n",
    "#     eval_dataset=capt_dataset[\"test\"],\n",
    "#     compute_metrics=capt_compute_metrics,\n",
    "#     preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    "# )\n",
    "\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capt_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset[89]\n",
    "image = sample['image']\n",
    "height,width = image.size\n",
    "display(image.resize((int(0.3*height),int(0.3*width))))\n",
    "desc = sample['description']\n",
    "print(f'Description: {desc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor_capt(images = image, return_tensors='pt').to(device)\n",
    "pixel_values = inputs.pixel_values\n",
    "\n",
    "generated_ids = model_capt.generate(pixel_values=pixel_values, max_length=5000)\n",
    "generated_caption = processor_capt.batch_decode(generated_ids,skip_special_tokens=True)[0]\n",
    "print(generated_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_capt.push_to_hub(\"Art_huggingface_caption\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m111"
  },
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

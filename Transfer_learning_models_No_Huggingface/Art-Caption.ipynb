{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%pip install torchtext\n",
    "%pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "from PIL import Image, ImageFile\n",
    "Image.LOAD_TRUNCATED_IMAGES = True\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../described_dataset_label.csv',sep='\\t',encoding='latin-1')\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "data = data[:20000]\n",
    "data = data.rename(columns={'FILE':'image','URL':'description'})\n",
    "data = data[['image','description']]\n",
    "data['image'] = [f'.{x}' for x in data['image']]\n",
    "print(f'columns:{[x for x in data.columns]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "img_tr = [transform(Image.open(img)) for img in tqdm(data['image'])]\n",
    "\n",
    "mean,std = img_tr[0].mean(),img_tr[0].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean,std = img_tr[0].mean([1,2]),img_tr[0].std([1,2])\n",
    "print(\"mean and std before normalize:\")\n",
    "print(\"Mean of the image:\", mean)\n",
    "print(\"Std of the image:\", std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train into train and val\n",
    "X_train, X_val, y_train, y_val = train_test_split(data['image'],data['description'], test_size=0.2, random_state=42)\n",
    "print(f'Train size:{len(X_train)} Val size:{len(y_train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image Transformations\n",
    "img_train_transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean,std),\n",
    "    ])\n",
    "\n",
    "img_test_transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean,std),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocabulary Class Explanation\n",
    "\n",
    "## Introduction\n",
    "This code defines a Python class called `Vocabulary` which is designed to create and manage a vocabulary for natural language processing tasks.\n",
    "\n",
    "## Class Initialization\n",
    "- The `Vocabulary` class is initialized with a name.\n",
    "- It initializes three tokens (`PAD_token`, `SOS_token`, `EOS_token`) which are used for padding short sentences, marking the start of a sentence, and marking the end of a sentence respectively.\n",
    "- It initializes dictionaries to store mappings between words and indices, and vice versa.\n",
    "- It also tracks the number of words, sentences, and the length of the longest sentence.\n",
    "\n",
    "## Method: add_word\n",
    "- This method adds a word to the vocabulary.\n",
    "- If the word is not already in the vocabulary, it assigns it a unique index and initializes its count to 1.\n",
    "- If the word already exists in the vocabulary, it simply increments its count.\n",
    "\n",
    "## Method: add_sentence\n",
    "- This method adds a sentence to the vocabulary.\n",
    "- It splits the sentence into words, adds each word using the `add_word` method, and updates the length of the longest sentence.\n",
    "- It also increments the count of sentences.\n",
    "\n",
    "## Method: to_word\n",
    "- This method converts an index to its corresponding word in the vocabulary.\n",
    "\n",
    "## Method: to_index\n",
    "- This method converts a word to its corresponding index in the vocabulary.\n",
    "\n",
    "## Method: len\n",
    "- This method returns the total number of words in the vocabulary.\n",
    "\n",
    "## Usage\n",
    "- After defining the `Vocabulary` class, an instance of it named `vocab` is created with the name 'art'.\n",
    "- Then, it iterates through sentences (presumably contained in a variable called `data['description']`), tokenizes each sentence into words using NLTK's `word_tokenize` function, and adds each word to the vocabulary using the `add_word` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "class Vocabulary:\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        PAD_token = 0   # Used for padding short sentences\n",
    "        SOS_token = 1   # Start-of-sentence token\n",
    "        EOS_token = 2   # End-of-sentence token\n",
    "        self.word2index = {\"PAD\": PAD_token, \"SOS\": SOS_token, \"EOS\": EOS_token}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3\n",
    "        self.num_sentences = 0\n",
    "        self.longest_sentence = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            # First entry of word into vocabulary\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            # Word exists; increase word count\n",
    "            self.word2count[word] += 1\n",
    "            \n",
    "    def add_sentence(self, sentence):\n",
    "        sentence_len = 0\n",
    "        for word in sentence.split(' '):\n",
    "            sentence_len += 1\n",
    "            self.add_word(word)\n",
    "        if sentence_len > self.longest_sentence:\n",
    "            # This is the longest sentence\n",
    "            self.longest_sentence = sentence_len\n",
    "        # Count the number of sentences\n",
    "        self.num_sentences += 1\n",
    "\n",
    "    def to_word(self, index):\n",
    "        return self.index2word[index]\n",
    "\n",
    "    def to_index(self, word):\n",
    "        return self.word2index[word]\n",
    "    \n",
    "    def len(self):\n",
    "        return self.num_words\n",
    "\n",
    "vocab = Vocabulary('art')\n",
    "for sentence in tqdm(data['description']):\n",
    "    sentence = word_tokenize(sentence)\n",
    "    for word in sentence:\n",
    "        vocab.add_word(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ArtDataset Class Explanation\n",
    "\n",
    "## Introduction\n",
    "This code defines a custom dataset class `ArtDataset` for handling image and caption data. It's designed to be compatible with PyTorch's `torch.utils.data.Dataset` class.\n",
    "\n",
    "## Class Initialization\n",
    "- The `ArtDataset` class is initialized with image data, corresponding labels (captions), an optional transformation for images, and a maximum caption length.\n",
    "- It stores these parameters as attributes for later use.\n",
    "\n",
    "## Method: __len__\n",
    "- This method returns the length of the dataset, which is the number of samples (images) in the dataset.\n",
    "\n",
    "## Method: __getitem__\n",
    "- This method is used to retrieve a single item (image and caption pair) from the dataset given an index.\n",
    "- It loads the image corresponding to the index using PIL's `Image.open` function and converts it to RGB format.\n",
    "- If a transformation is provided, it applies the transformation to the image.\n",
    "- It tokenizes the caption using NLTK's `word_tokenize` function, appending start-of-sentence (`SOS`) and end-of-sentence (`EOS`) tokens to the tokenized caption.\n",
    "- If the length of the tokenized caption exceeds the maximum caption length, it truncates it; otherwise, it pads it with `PAD` tokens to match the maximum length.\n",
    "- It then converts the tokens to indices using a predefined vocabulary (`vocab`) and creates a PyTorch tensor from the indices.\n",
    "- Finally, it returns a tuple containing the processed image and the tokenized caption.\n",
    "\n",
    "## Dataset Instantiation\n",
    "- After defining the `ArtDataset` class, two instances of it (`train_dataset` and `test_dataset`) are created using training and validation data (`X_train`, `y_train`, `X_val`, `y_val`), respectively.\n",
    "- From the `train_dataset` a validation dataset is further extracted.\n",
    "- Transformation functions (`img_train_transform` and `img_test_transform`) are applied to the images if provided.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "class ArtDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self,images,labels,transform=None, max_caption_length=50):\n",
    "        self.data = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.max_caption_length = max_caption_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        img_pil = Image.open(self.data[index])\n",
    "        img_pil = img_pil.convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            img_pil = self.transform(img_pil)\n",
    "        else:\n",
    "            tr1 = transforms.ToTensor()\n",
    "            img_pil = tr1(img_pil)\n",
    "\n",
    "        #Tokenize caption using nltk\n",
    "        caption = self.labels[index]\n",
    "        tokens = []\n",
    "        tokens.append('SOS')\n",
    "        tokens.extend(word_tokenize(caption))\n",
    "        tokens.append('EOS')\n",
    "        if len(tokens) > self.max_caption_length:\n",
    "            tokens = tokens[:self.max_caption_length]\n",
    "        else:\n",
    "            tokens += [\"PAD\"] * (self.max_caption_length - len(tokens))\n",
    "        #Convert tokens to indices\n",
    "        caption = [vocab.to_index(token) for token in tokens]\n",
    "\n",
    "        tokenized_caption = torch.LongTensor(caption)\n",
    "        return(img_pil,tokenized_caption)\n",
    "\n",
    "train_dataset = ArtDataset(X_train.values,y_train.values,transform=img_train_transform)\n",
    "val_dataset = ArtDataset(X_train.values,y_train.values,transform=img_test_transform)\n",
    "test_dataset = ArtDataset(X_val.values,y_val.values,transform=img_test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data\n",
    "del X_train,X_val,y_train,y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Explanation\n",
    "\n",
    "The `Encoder` class is responsible for extracting meaningful features from input data, typically images, and transforming them into a format suitable for further processing by the decoder.\n",
    "\n",
    "### Initialization\n",
    "- **Embedding Dimension (`embed_dim`)**: This parameter specifies the dimensionality of the output feature vectors produced by the encoder.\n",
    "- **Batch Normalization Momentum (`batch_norm_momentum`)**: Momentum value used for batch normalization.\n",
    "- **Dropout Probability (`dropout`)**: Dropout probability for regularization.\n",
    "\n",
    "### Components\n",
    "1. **DenseNet-201 Backbone**: The encoder utilizes the DenseNet-201 architecture, a deep convolutional neural network pre-trained on the ImageNet dataset. DenseNet-201 consists of densely connected blocks, enabling feature reuse and efficient feature propagation.\n",
    "2. **Freezing Pre-trained Weights**: The parameters of the DenseNet-201 backbone are frozen (`requires_grad = False`), preventing them from being updated during training to retain the learned features.\n",
    "3. **Linear Layer (`embed`)**: Adjusts the output features from the DenseNet-201 classifier to the desired embedding dimension (`embed_dim`). This layer reduces the dimensionality of the feature vectors.\n",
    "4. **Batch Normalization (`batch`)**: Applies batch normalization to normalize the output features and improve training stability. It adjusts the mean and variance of each feature dimension.\n",
    "5. **Dropout Layer**: Randomly drops a fraction of the input features during training to prevent overfitting and improve generalization.\n",
    "6. **Initialization**: Weights of the linear layer are initialized with normal distribution (mean=0, standard deviation=0.1), and bias is initialized to zeros.\n",
    "\n",
    "### Forward Pass\n",
    "- **Input Processing**: Takes images (`imgs`) as input.\n",
    "- **Feature Extraction**: Passes the input images through the DenseNet-201 backbone to extract high-level features. The DenseNet architecture efficiently captures spatial hierarchies in the images.\n",
    "- **Dropout Regularization**: Applies dropout to the extracted features to prevent overfitting by randomly setting a fraction of the features to zero during training.\n",
    "- **Dimensionality Reduction**: Projects the extracted features to the desired embedding dimension using a linear layer. This step reduces the dimensionality of the feature vectors while preserving relevant information.\n",
    "- **Batch Normalization**: Normalizes the output features using batch normalization, ensuring stable training by scaling and shifting the features.\n",
    "- **Output**: Returns the processed features as the output of the encoder.\n",
    "\n",
    "### Summary\n",
    "The `Encoder` class employs the DenseNet-201 architecture to extract informative features from input images. By reducing dimensionality, applying batch normalization, and employing dropout regularization, the encoder prepares the features for further processing by the decoder, facilitating the generation of accurate and descriptive captions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.densenet import DenseNet201_Weights\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embed_dim, batch_norm_momentum=0.9, dropout=0.5) -> None:\n",
    "        super(Encoder,self).__init__()\n",
    "        # resnet = torchvision.models.resnet50(weights=\"ResNet50_Weights.DEFAULT\")\n",
    "        # resnet = torchvision.models.resnext101_64x4d(pretrained=True)\n",
    "        self.dense_net = torchvision.models.densenet201(weights=DenseNet201_Weights.IMAGENET1K_V1)\n",
    "        for param in self.dense_net.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        #Adjusting the output features according to the embed_dim\n",
    "        self.embed = nn.Linear(self.dense_net.classifier.out_features,embed_dim)\n",
    "        self.batch = nn.BatchNorm1d(embed_dim,momentum=batch_norm_momentum)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        #Initializing the weights and bias\n",
    "        self.embed.weight.data.normal_(0.0,0.1)\n",
    "        self.embed.bias.data.fill_(0)\n",
    "\n",
    "\n",
    "    def forward(self,imgs):\n",
    "        features = self.dense_net(imgs)\n",
    "        features = self.dropout(features)\n",
    "        features = self.embed(features)\n",
    "        features = self.batch(features)\n",
    "        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Explanation\n",
    "\n",
    "The `Attention` class enables the model to selectively focus on different parts of the input sequence (or image features) when generating each element of the output sequence.\n",
    "\n",
    "### Initialization\n",
    "- **Encoder Dimension (`encoder_dim`)**: The dimensionality of the encoder output features.\n",
    "- **Decoder Dimension (`decoder_dim`)**: The dimensionality of the decoder hidden states.\n",
    "- **Attention Dimension (`attention_dim`)**: The dimensionality of the attention mechanism.\n",
    "\n",
    "### Components\n",
    "1. **Linear Layers**: The attention mechanism consists of three linear transformations:\n",
    "   - `encoder_att`: Projects the encoder output features to the attention dimension.\n",
    "   - `decoder_att`: Projects the decoder hidden states to the attention dimension.\n",
    "   - `full_att`: Projects the concatenated attention vectors to a single value, representing the attention score.\n",
    "\n",
    "2. **Forward Pass**:\n",
    "   - **Encoder Attention**: Projects the encoder output features (`encoder_out`) using the `encoder_att` linear layer.\n",
    "   - **Decoder Attention**: Projects the decoder hidden state (`decoder_hidden`) using the `decoder_att` linear layer and adds a singleton dimension.\n",
    "   - **Combination**: Combines the projected encoder and decoder attention vectors and applies the Rectified Linear Unit (ReLU) activation function to obtain the attention scores.\n",
    "   - **Attention Score**: Projects the combined attention vectors using the `full_att` linear layer and squeezes the resulting tensor to obtain a single attention score for each element in the input sequence.\n",
    "   - **Attention Weights**: Applies the softmax function along the sequence dimension to obtain the attention weights (`alpha`). These weights represent the importance of each input feature at the current decoding step.\n",
    "   - **Weighted Sum**: Computes the weighted sum of the encoder output features using the attention weights to obtain the attention-weighted encoding (`attention_weighted_encoding`).\n",
    "\n",
    "### Summary\n",
    "The `Attention` class enables the model to dynamically focus on different parts of the input sequence (or image features) during the decoding process. By assigning higher weights to relevant features and lower weights to less relevant ones, the attention mechanism enhances the model's ability to generate accurate and contextually relevant outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim) -> None:\n",
    "        super(Attention, self).__init__()\n",
    "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)#U\n",
    "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)#W\n",
    "        self.full_att = nn.Linear(attention_dim,1)#V\n",
    "        \n",
    "\n",
    "    def forward(self, encoder_out, decoder_hidden):\n",
    "        att1 = self.encoder_att(encoder_out)\n",
    "        att2 = self.decoder_att(decoder_hidden).unsqueeze(1) # W * decoder_hidden\n",
    "        att = F.relu(att1+att2) # ReLU(U * encoder_out + W * decoder_hidden)\n",
    "        att = self.full_att(att).squeeze(2) # V * ReLU(U * encoder_out + W * decoder_hidden)\n",
    "        alpha = F.softmax(att, dim=1) # attention weights\n",
    "        attention_weighted_encoding = torch.sum(encoder_out * alpha,dim=1).squeeze(1) # attention_weighted_encoding\n",
    "\n",
    "        return attention_weighted_encoding, alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Explanation\n",
    "\n",
    "The `Decoder` class take the encoded image features, often extracted by the encoder, and transform them into descriptive text captions.\n",
    "\n",
    "### Initialization\n",
    "- **Embedding Size (`embed_size`)**: This parameter defines the size of the word embedding vectors. Word embeddings are dense, continuous-valued representations of words in a vector space.\n",
    "- **Hidden Size (`hidden_size`)**: This parameter specifies the size of the hidden state in the LSTM (Long Short-Term Memory) layer. The hidden state captures information from previous time steps.\n",
    "- **Vocabulary Size (`vocab_size`)**: The size of the vocabulary, i.e., the total number of unique words in the language.\n",
    "- **Dropout Probability (`dropout`)**: Dropout is a regularization technique that randomly sets a fraction of input units to zero during training to prevent overfitting.\n",
    "\n",
    "### Components\n",
    "1. **Attention Mechanism**: The decoder utilizes an attention mechanism to selectively focus on different regions of the input image features while generating captions. This allows the model to attend to relevant parts of the image during each step of caption generation.\n",
    "2. **Embedding Layer**: Converts word indices into dense vectors of fixed size (`embed_size`). This layer essentially learns the semantic representations of words based on their context in the captions.\n",
    "3. **LSTM Layer**: The LSTM processes sequential input data and maintains hidden states across time steps. It takes embedded word vectors as input and produces output sequences. The hidden state captures context information and helps in capturing long-range dependencies.\n",
    "4. **Linear Layer**: Maps the output of the LSTM to a space with dimensionality equal to the vocabulary size. This layer produces logits for each word in the vocabulary, representing the likelihood of each word being the next word in the sequence.\n",
    "5. **Sigmoid Layer**: A sigmoid function used to compute the gating mechanism for attention. It helps in controlling the attention mechanism, determining the relevance of different parts of the image features.\n",
    "6. **Initialization**: Weights for the embedding and linear layers are initialized within a certain range to ensure effective learning during training.\n",
    "\n",
    "### Forward Pass\n",
    "- **Input Processing**: Takes image features (`features`) and ground truth captions (`captions`) as inputs.\n",
    "- **Embedding and Concatenation**: Embeds the captions using the embedding layer and concatenates them with the image features. This combined representation is used as input for subsequent processing.\n",
    "- **LSTM Computation**: Passes the concatenated input through the LSTM, computing attention-weighted encoding. This step captures the contextual information from both the image features and the previously generated words in the captions.\n",
    "- **Attention Mechanism**: Applies a gating mechanism to the attention-weighted encoding, determining the importance of different parts of the image features in generating the next word.\n",
    "- **Output Generation**: Generates logits for each word in the vocabulary using a linear layer. These logits represent the model's confidence in predicting each word in the vocabulary.\n",
    "- **Regularization**: Applies dropout regularization to the output sequence to prevent overfitting during training.\n",
    "\n",
    "### Sampling Methods\n",
    "- **Greedy Sampling (`sample`)**: Selects the word with the highest probability at each time step to form the sequence. This method tends to produce coherent but potentially repetitive captions.\n",
    "- **Combined Sampling (`sample_combined_search`)**: Combines the sampling from top-k and nucleus sampling methods to enhance diversity and reduce repetition in the generated captions. \n",
    "- **Top-k Sampling (`top_k_sampling`)**: Samples from the top k most likely words at each time step, enhancing diversity in the generated captions.\n",
    "- **Nucleus Sampling (`nucleus_sampling`)**: Samples from the set of words whose cumulative probability mass exceeds a certain threshold (`p`). This method promotes diversity while controlling the number of possible words sampled.\n",
    "\n",
    "### Summary\n",
    "The `Decoder` class is a fundamental component of image captioning models, responsible for converting visual information into textual descriptions. Through a combination of attention mechanisms, recurrent neural networks, and sampling techniques, the decoder effectively generates captions that capture the essence of the input images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size,dropout=0.5):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention = Attention(embed_size, hidden_size, hidden_size)\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.f_beta = nn.Linear(hidden_size, embed_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.linear.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.linear.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        embeddings = self.embedding(captions)\n",
    "        features = features.unsqueeze(1)\n",
    "        embeddings = torch.cat((features, embeddings[:,:-1,:]), dim=1)\n",
    "        hidden, _ = self.lstm(embeddings)\n",
    "        attention_weighted_encoding, _ = self.attention(features, hidden)\n",
    "        gate = self.sigmoid(self.f_beta(hidden))\n",
    "        attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "        # inputs = torch.cat((embeddings[:, :-1, :], attention_weighted_encoding), dim=1) #Combined lstm output and attention weighted encoding in a single tensor, reduces the number of LSTM iterations.\n",
    "        inputs = torch.cat((embeddings,attention_weighted_encoding))\n",
    "        lstm_out, _ = self.lstm(inputs)\n",
    "        \n",
    "        outputs = self.linear(self.dropout(lstm_out))\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def sample(self,features, max_len=50):\n",
    "        sampled_ids = []\n",
    "        inputs = features.unsqueeze(1)\n",
    "        states = None\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            hiddens, states = self.lstm(inputs, states)          \n",
    "            outputs = self.linear(hiddens)\n",
    "            out = outputs.squeeze(1)    \n",
    "            predicted = torch.argmax(out,dim=1)\n",
    "            sampled_ids.append(predicted.cpu().numpy()[0].item())\n",
    "            inputs = self.embedding(predicted).unsqueeze(1)                 \n",
    "        # sampled_ids = torch.stack(sampled_ids,1)\n",
    "        return sampled_ids\n",
    "    \n",
    "    #Top-k + Nucleus Sampling\n",
    "    def sample_combined_search(self, features, max_len=50, temperature=1.0, top_k=0, top_p=0.0):\n",
    "        sampled_ids = []\n",
    "        inputs = features.unsqueeze(1)\n",
    "        states = None\n",
    "        \n",
    "        for _ in range(max_len):\n",
    "            hiddens, states= self.lstm(inputs, states)\n",
    "            outputs = self.linear(hiddens)/temperature\n",
    "            if top_k > 0:\n",
    "                outputs = self.top_k_sampling(outputs, top_k)\n",
    "            elif top_p < 1.0:\n",
    "                outputs = self.nucleus_sampling(outputs,top_p)\n",
    "            out = outputs.squeeze(1)    \n",
    "            predicted = torch.argmax(out,dim=1)\n",
    "            sampled_ids.append(predicted.cpu().numpy()[0].item())\n",
    "            inputs = self.embedding(predicted).unsqueeze(1) \n",
    "        # sampled_ids = torch.stack(sampled_ids,1)\n",
    "        return sampled_ids\n",
    "\n",
    "#Operates directly on logits and not softmax probabilities\n",
    "    def top_k_sampling(self, logits, k):\n",
    "        with torch.no_grad():\n",
    "            top_values, _ = torch.topk(logits, k, dim=-1)\n",
    "            min_value = top_values[:, -1].unsqueeze(-1)\n",
    "            logits = torch.where(logits < min_value, torch.tensor(-float('inf'), device=logits.device), logits)\n",
    "        return logits\n",
    "\n",
    "    def nucleus_sampling(self, logits, p):\n",
    "        with torch.no_grad():\n",
    "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "            sorted_indices_to_remove = cumulative_probs > p\n",
    "            sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n",
    "            sorted_indices_to_remove[:, 0] = 0\n",
    "            indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "            logits[indices_to_remove] = -float('inf')\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import PyTorchModelHubMixin #Allows to save model to hub and have some functionalities of huggingface models\n",
    "\n",
    "class ArtModel(nn.Module, PyTorchModelHubMixin):\n",
    "    def __init__(self,embed_size,hidden_size, vocab_size, batch_norm_momentum=0.9, dropout=0.5):\n",
    "        super(ArtModel,self).__init__()\n",
    "        self.encoder = Encoder(embed_size, batch_norm_momentum=batch_norm_momentum, dropout=dropout)\n",
    "        self.decoder = Decoder(embed_size=embed_size, hidden_size=hidden_size, vocab_size=vocab_size,dropout=dropout)\n",
    "\n",
    "    def forward(self,imgs,captions):\n",
    "        features = self.encoder(imgs)\n",
    "        outputs = self.decoder(features,captions)\n",
    "        return outputs\n",
    "\n",
    "    def sample(self,imgs,max_len=50):\n",
    "        features = self.encoder(imgs)\n",
    "        sampled_ids = self.decoder.sample(features,max_len)\n",
    "        return sampled_ids\n",
    "\n",
    "    def sample_combined_search(self,imgs,max_len=50,temperature=1.0,top_k=1,top_p=0.9):\n",
    "        features = self.encoder(imgs)\n",
    "        sampled_ids = self.decoder.sample_combined_search(features,max_len,temperature,top_k,top_p)\n",
    "        return sampled_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `train` Function Explanation\n",
    "\n",
    "## Introduction\n",
    "This function is responsible for training the Show and Tell model using the provided data and optimizer.\n",
    "\n",
    "## Function Parameters\n",
    "- `train_loader`: The data loader for training images and captions.\n",
    "- `encoder`: The encoder model responsible for extracting image features.\n",
    "- `decoder`: The decoder model responsible for generating captions.\n",
    "- `criterion`: The loss function used to compute the training loss.\n",
    "- `encoder_optimizer`: The optimizer used to update the parameters of the encoder model (can be `None` if the encoder is frozen).\n",
    "- `decoder_optimizer`: The optimizer used to update the parameters of the decoder model.\n",
    "- `epoch`: The current epoch number.\n",
    "\n",
    "## Training Loop\n",
    "- The function iterates over the batches of training data provided by `train_loader`.\n",
    "- For each batch:\n",
    "  - It moves the images and captions to the appropriate device (CPU or GPU).\n",
    "  - It computes the lengths of the captions in the batch.\n",
    "  - It passes the images through the encoder to obtain image features.\n",
    "  - It passes the image features and captions through the decoder to obtain predicted scores for each word in the captions.\n",
    "  - It packs the predicted scores and the ground truth captions using `pack_padded_sequence` to handle variable-length sequences.\n",
    "  - It computes the loss between the predicted scores and the ground truth captions using the provided loss function (`criterion`).\n",
    "  - It backpropagates the loss and updates the parameters of the decoder (and optionally the encoder) using the specified optimizers (`decoder_optimizer` and `encoder_optimizer`).\n",
    "  - It logs the current loss for monitoring training progress.\n",
    "  - It updates the progress bar with information about the current epoch, batch number, and loss.\n",
    "\n",
    "## Logging\n",
    "- We log the training loss for each batch to monitor the training progress.\n",
    "- We update the progress bar to display the current epoch, batch number, and loss information.\n",
    "- We return the Training loss and validation loss for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,encoder_optimizer, decoder_optimizer, criterion, train_loader, val_loader, epochs=10, early_stopping_patience=25):\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    #Train the model\n",
    "    with tqdm(total=epochs) as pbar:\n",
    "        for epoch in range(epochs):\n",
    "            model.encoder.train()\n",
    "            model.decoder.train()\n",
    "            train_loss = 0.0\n",
    "            for _, (images, captions) in enumerate(train_loader,0):\n",
    "                images, captions = images.to(device), captions.to(device)\n",
    "                encoder_optimizer.zero_grad() if encoder_optimizer else None\n",
    "                decoder_optimizer.zero_grad()\n",
    "                \n",
    "                outputs = model(images, captions)\n",
    "\n",
    "                #Flatten the outputs and the captions\n",
    "                decode_lengths = [len(cap) for cap in captions]\n",
    "                outputs = nn.utils.rnn.pack_padded_sequence(outputs, decode_lengths, batch_first=True, enforce_sorted=False)\n",
    "                captions = nn.utils.rnn.pack_padded_sequence(captions, decode_lengths, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "                loss = criterion(outputs.data,captions.data)\n",
    "                \n",
    "                loss.backward()\n",
    "                encoder_optimizer.step() if encoder_optimizer else None\n",
    "                decoder_optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "\n",
    "            train_losses.append(train_loss/len(train_loader))\n",
    "\n",
    "            #Validation of the model\n",
    "            model.encoder.eval()\n",
    "            model.decoder.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for img,captions in val_loader:\n",
    "                    img,captions = img.to(device),captions.to(device)\n",
    "\n",
    "                    outputs = model(img,captions)\n",
    "                    \n",
    "                    decode_lengths = [len(cap) for cap in captions]\n",
    "                    outputs = nn.utils.rnn.pack_padded_sequence(outputs, decode_lengths, batch_first=True, enforce_sorted=False)\n",
    "                    captions = nn.utils.rnn.pack_padded_sequence(captions, decode_lengths, batch_first=True, enforce_sorted=False)\n",
    "                    loss = criterion(outputs.data,captions.data)\n",
    "                    val_loss += loss.item()\n",
    "            val_losses.append(val_loss/len(val_loader))\n",
    "\n",
    "            #Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve >= early_stopping_patience:\n",
    "                    print(f'Early stopping! Epoch: {epoch+1}')\n",
    "                    break\n",
    "\n",
    "            pbar.set_description(f'Train Loss: {train_losses[-1]:.3f} | Val Loss: {val_losses[-1]:.3f}')\n",
    "            pbar.update(1)\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss(loss_values, title=\"Loss Plot\"):\n",
    "    \"\"\"\n",
    "    Plot the loss values over training epochs.\n",
    "\n",
    "    Parameters:\n",
    "    - loss_values: A list or array of loss values for each epoch.\n",
    "    - title: The title of the plot (optional).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(loss_values[0], marker='o', linestyle='-', label='train')\n",
    "    plt.plot(loss_values[1], marker='o', linestyle='-',label='val')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `convert_to_words` Function Explanation\n",
    "\n",
    "## Introduction\n",
    "This function converts the predicted word indices generated by the decoder into actual words using the vocabulary mapping.\n",
    "\n",
    "## Function Parameters\n",
    "- `sampled_ids`: A list of predicted word indices generated by the decoder.\n",
    "\n",
    "## Conversion Process\n",
    "- The function iterates over the predicted word indices (`sampled_ids`).\n",
    "- For each word index, it retrieves the corresponding word from the vocabulary using the `index2word` mapping.\n",
    "- It appends the word to the `sampled_caption` list.\n",
    "- If the word is 'EOS' (end-of-sentence token), it stops the iteration as the caption is complete.\n",
    "- Finally, it joins the words in the `sampled_caption` list to form the final caption sentence.\n",
    "\n",
    "## Summary\n",
    "This function plays a crucial role in converting the output of the decoder (predicted word indices) into human-readable captions. It utilizes the vocabulary mapping (`index2word`) to perform the conversion, enabling the model to generate descriptive captions for images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the function to convert the predicted ids to words\n",
    "def convert_to_words(sampled_ids):\n",
    "    sampled_caption = []\n",
    "    for word_id in sampled_ids:\n",
    "        word_id = int(word_id)\n",
    "        word = vocab.index2word[word_id]\n",
    "        sampled_caption.append(word)\n",
    "        if word == 'EOS':\n",
    "            break\n",
    "    sentence = ' '.join(sampled_caption)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the prediction function\n",
    "def predict(model,image, top_k, top_p, max_len=50, temperature=1.0):\n",
    "    model.encoder.eval()\n",
    "    model.decoder.eval()\n",
    "    image = image.to(device)\n",
    "    sampled_ids = []\n",
    "    sampled_ids = model.sample_combined_search(image, max_len=max_len, temperature=temperature, top_k=top_k, top_p=top_p)\n",
    "    return sampled_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROUGE Score Computation Function Explanation\n",
    "\n",
    "This function computes the ROUGE (Recall-Oriented Understudy for Gisting Evaluation) score for the generated captions by the Show and Tell model on the validation dataset.\n",
    "\n",
    "## Function Parameters\n",
    "- `testloader`: The data loader for validation images and captions.\n",
    "- `encoder`: The encoder model used for feature extraction.\n",
    "- `decoder`: The decoder model used for caption generation.\n",
    "\n",
    "## ROUGE Score Computation\n",
    "- The function imports the `load_metric` function from the `datasets` library, which is used to load the ROUGE metric.\n",
    "- It sets the encoder and decoder models to evaluation mode using the `eval()` method.\n",
    "- The function iterates over the batches of validation data provided by `testloader`.\n",
    "- For each batch:\n",
    "  - It moves the images to the appropriate device (CPU or GPU).\n",
    "  - It extracts features from the images using the encoder.\n",
    "  - It generates captions for the images using the decoder's `sample_combined_search` method, which combines `top_k` and `nucleus` sampling.\n",
    "  - It converts the indices of the generated captions to actual words using the vocabulary (`vocab`).\n",
    "  - It adds the generated captions and ground truth captions to the ROUGE metric using the `add` method.\n",
    "- Finally, the function computes the ROUGE score using the `compute` method of the ROUGE metric.\n",
    "\n",
    "## ROUGE Score\n",
    "- The ROUGE score is a metric commonly used to evaluate the quality of machine-generated text summaries. It measures the overlap of n-grams (typically up to 4-grams) between the generated captions and the reference captions, focusing on recall rather than precision.\n",
    "\n",
    "## Summary\n",
    "This function evaluates the quality of the generated captions by computing the ROUGE score, providing a quantitative measure of the model's performance in generating captions that capture the essence of the ground truth captions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_metric\n",
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load('rouge')\n",
    "def rouge_score(model, data_loader, top_p, top_k):\n",
    "    predictions = []\n",
    "    references = []\n",
    "    for img, label in data_loader: \n",
    "        res = predict(model, img, top_k=top_k, top_p=top_p)\n",
    "        sentence = convert_to_words(res)\n",
    "        predictions.append(sentence)\n",
    "        references.append([convert_to_words(l) for l in label])\n",
    "    result = rouge.compute(predictions=predictions, references=references, use_aggregator=True)\n",
    "    return result['rougeL']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing the Model with Optuna\n",
    "\n",
    "Optuna is an automatic hyperparameter optimization framework. This section covers the integration of Optuna for optimizing the hyperparameters of the Show and Tell model.\n",
    "\n",
    "#### Steps:\n",
    "1. Define an objective function that returns the value to be optimized.\n",
    "2. Use `optuna.create_study()` to create a study object.\n",
    "3. Call `study.optimize()` with the objective function and the number of trials.\n",
    "\n",
    "#### Objective Function:\n",
    "- The objective function trains the model with a set of hyperparameters suggested by Optuna.\n",
    "- It evaluates the model's performance on the validation set and returns the evaluation metric.\n",
    "\n",
    "#### Study Creation:\n",
    "- `study = optuna.create_study(direction='maximize')`: Creates a study that aims to maximize the evaluation metric.\n",
    "\n",
    "#### Optimization:\n",
    "- `study.optimize(objective, n_trials=100)`: Runs the optimization for 100 trials.\n",
    "\n",
    "### Example Code:\n",
    "```python\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    # Define hyperparameters to tune\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 128, step=16)\n",
    "    # Model training and evaluation code\n",
    "    ...\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import optuna\n",
    "def objective(trial):\n",
    "    tmp_score = 0\n",
    "    # Define search space for hyperparameters\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1,log=True, step=1e-5)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64])\n",
    "    epochs = trial.suggest_int('epochs', 5, 50, step=5)\n",
    "    early_stopping_patience = trial.suggest_int('early_stopping_patience', 5, epochs//2)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-4, log=True, step=1e-6)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', low=0.1, high=0.5, step=0.1)\n",
    "    top_k = trial.suggest_categorical('top_k', [0, 1])\n",
    "    top_p = trial.suggest_float('top_p', low=0.0, high=0.9, step=0.1)\n",
    "\n",
    "    # Train the model with current hyperparameters\n",
    "\n",
    "    model = ArtModel(embed_size=512, hidden_size=512, vocab_size=vocab.len(),dropout=dropout_rate).to(device)\n",
    "    optimizer_encoder = torch.optim.AdamW(model.encoder.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    optimizer_decoder = torch.optim.AdamW(model.decoder.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    _,val_loss = train(model, optimizer_encoder, optimizer_decoder, criterion, train_loader, val_loader, epochs, early_stopping_patience)\n",
    "    score = rouge_score(model, val_loader, top_p=top_p, top_k=top_k)\n",
    "    study.set_user_attr(key=\"model\", value=model)\n",
    "    return score\n",
    "\n",
    "def callback(study, trial):\n",
    "    if study.best_trial.number == trial.number: \n",
    "        os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
    "        model = study.user_attrs[\"model\"]\n",
    "        torch.save(model.state_dict(), './model_checkpoints/model_capt.pth')\n",
    "\n",
    "# Create a study object and optimize the objective function\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100, callbacks=[callback])\n",
    "\n",
    "# Get the best hyperparameters and accuracy\n",
    "best_params = study.best_params\n",
    "best_accuracy = study.best_value\n",
    "\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "print(\"Best score:\", best_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Hyperparameters\n",
    "\n",
    "### Training Process:\n",
    "1. Load and preprocess the data.\n",
    "2. Define the model architecture for both the encoder and decoder.\n",
    "3. Set up the loss function and optimizer.\n",
    "4. Train the model using the training dataset.\n",
    "5. Validate the model using the validation dataset.\n",
    "6. Monitor the training process using metrics such as loss and accuracy.\n",
    "\n",
    "### Hyperparameters:\n",
    "- Learning Rate: Determines the step size during gradient descent.\n",
    "- Batch Size: Number of samples processed before updating the model.\n",
    "- Epochs: Number of times the entire training dataset is passed through the model.\n",
    "- Optimizer: Algorithm used for updating model parameters (e.g., Adam, SGD).\n",
    "\n",
    "### Example Code:\n",
    "```python\n",
    "# Example of setting hyperparameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "epochs = 20\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    for images, captions in trainloader:\n",
    "        # Training step\n",
    "        ...\n",
    "    # Validation step\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not best_params:\n",
    "    best_params = {'learning_rate': 0.005427500093408814, 'batch_size': 64, 'early_stopping_patience': 5, 'weight_decay': 2.8187340750251974e-06, 'dropout_rate': 0.29510686802660724, 'top_k': 1, 'top_p': 0.36006145902967357, 'epochs': 23}\n",
    "batch_size = best_params['batch_size']\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "testloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "features, labels = next(iter(trainloader))\n",
    "print(f\"Feature batch shape: {features.size()}\")\n",
    "print(f\"Labels batch shape: {len(labels)}\")\n",
    "img = features[0].squeeze()\n",
    "plt.imshow(img.permute(*torch.arange(img.ndim - 1, -1, -1)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters and Hyperparameters\n",
    "#Model Parameters\n",
    "embed_dim = 512\n",
    "decoder_dim = 512\n",
    "dropout = best_params['dropout_rate']\n",
    "batch_norm_momentum = 0.9\n",
    "\n",
    "#Training Hyperparameters\n",
    "epochs = best_params['epochs']\n",
    "encoder_lr = best_params['learning_rate']\n",
    "decoder_lr = best_params['learning_rate']\n",
    "weight_decay = best_params['weight_decay']\n",
    "fine_tune_encoder = False\n",
    "checkpoint = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ArtModel(embed_size=embed_dim,hidden_size=decoder_dim,vocab_size=vocab.len(),batch_norm_momentum=batch_norm_momentum,dropout=dropout).to(device)\n",
    "\n",
    "parameters = list(model.decoder.parameters())+list(model.encoder.embed.parameters())\n",
    "decoder_optimizer = optim.AdamW(filter(lambda p: p.requires_grad, parameters), lr=decoder_lr, weight_decay=weight_decay)\n",
    "encoder_optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.encoder.parameters()), lr=encoder_lr, weight_decay=weight_decay) if fine_tune_encoder else None\n",
    " \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train_loss, val_loss = train(model,encoder_optimizer,decoder_optimizer,criterion,trainloader,valloader,epochs=epochs, early_stopping_patience=best_params['early_stopping_patience'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss([train_loss,val_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = rouge_score(model,testloader,top_p=0.9,top_k=1)\n",
    "print(f\"Rouge Score: {results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   #Define the function to display the image and the predicted caption\n",
    "def display_image_caption(image,sentence):\n",
    "    plt.imshow(image.squeeze().permute(1,2,0))\n",
    "    plt.title(sentence)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the function to display the image and the actual caption\n",
    "def display_image_caption_actual(image, label):\n",
    "    plt.imshow(image.squeeze().permute(1,2,0))\n",
    "    plt.title(convert_to_words(tuple(label.numpy())))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#img, label = next(iter(testloader))\n",
    "for i, (img, label) in enumerate(testloader):\n",
    "    # display_image_caption_actual(img[:1], label[:1])\n",
    "    plt.imshow(img.squeeze().permute(1,2,0))\n",
    "    img = img.to(device)\n",
    "    res = predict(model,img, top_k=0, top_p=0.5, max_len=500, temperature=0.5)\n",
    "    sentence = convert_to_words(res)\n",
    "    print(f\"Predicted: {sentence}\")\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
    "torch.save(model.state_dict(), './model_checkpoints/model_capt.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m111"
  },
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%pip install datasets\n",
    "%pip install transformers\n",
    "%pip install sentencepiece\n",
    "%pip install diffusers --upgrade\n",
    "%pip install invisible_watermark accelerate safetensors\n",
    "%pip install accelerate\n",
    "%pip install evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification Model\n",
    "This notebook implements a model for classifying art pieces. We use a visual Transformer model, which processes both the image and its corresponding label to provide a classification output.\n",
    "\n",
    "## Overview\n",
    "1. **Label Tokenization**: Convert text labels into numerical indices.\n",
    "2. **Image Processing**: Standardize and preprocess images for model input.\n",
    "3. **Model Architecture**: Define and train the visual Transformer model.\n",
    "4. **Evaluation**: Assess model performance using appropriate metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFile\n",
    "Image.LOAD_TRUNCATED_IMAGES = True\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import optuna\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Tokenization\n",
    "To facilitate feeding labels into the model, we tokenize them by pairing each unique label with a corresponding index. This process converts textual labels into numerical form, making them compatible with the model's input requirements.\n",
    "\n",
    "**Example**:\n",
    "- **Label**: \"Leonardo DaVinci\" -> **ID**: 3\n",
    "\n",
    "**Code**:\n",
    "```python\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Sample labels\n",
    "labels = [\"Leonardo DaVinci\", \"Vincent van Gogh\", \"Pablo Picasso\"]\n",
    "label_encoder = LabelEncoder()\n",
    "label_ids = label_encoder.fit_transform(labels)\n",
    "\n",
    "print(label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../described_dataset_label.csv',sep='\\t',encoding='utf-8')\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "data = data[:20000]\n",
    "data = data.rename(columns={'FILE':'image','AUTHOR':'author', 'TECHNIQUE':'style'})\n",
    "data = data[['image','author','style']]\n",
    "data['image'] = [f'.{x}' for x in data['image']]\n",
    "data['author'] = [x.lower() for x in data['author']]\n",
    "data['style'] = [x.split(',')[0].lower() for x in data['style']]\n",
    "\n",
    "with open('../label_author.json', 'r') as f:\n",
    "    labels_author = json.load(f)\n",
    "label2id_auth, id2label_auth = dict(), dict()\n",
    "for i, label in labels_author.items():\n",
    "    i= int(i)\n",
    "    id2label_auth[i]=label\n",
    "    label2id_auth[label]=i\n",
    "\n",
    "with open('../label_style.json', 'r') as f:\n",
    "    labels_sty = json.load(f)\n",
    "label2id_sty, id2label_sty = dict(), dict()\n",
    "for i, label in labels_sty.items():\n",
    "    i=int(i)\n",
    "    label2id_sty[label]=i\n",
    "    id2label_sty[i]=label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['author'] = data['author'].map(label2id_auth)\n",
    "data['style'] = data['style'].map(label2id_sty)\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Processing\n",
    "To achieve optimal results, we need to preprocess the images. Key steps include:\n",
    "\n",
    "#### Standardization\n",
    "Resize all images to a uniform size (e.g., 256x256 pixels) to ensure consistency in model input.\n",
    "\n",
    "#### Mean and Standard Deviation Calculation\n",
    "Calculate the mean and standard deviation of the images for normalization purposes. Normalization helps in stabilizing and speeding up the training process.\n",
    "\n",
    "#### Transformation Pipeline\n",
    "Use the `Compose` function from the `transforms` library to build a pipeline that applies a series of transformations to each image systematically.\n",
    "\n",
    "### Transformation Details\n",
    "Separate transformation pipelines are defined for training and testing datasets to include operations such as resizing, cropping, normalization, and data augmentation.\n",
    "\n",
    "**Code**:\n",
    "```python\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define transformation pipelines\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(256),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "img_tr = [transform(Image.open(img)) for img in tqdm(data['image'])]\n",
    "\n",
    "mean,std = img_tr[0].mean(),img_tr[0].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean,std = img_tr[0].mean([1,2]),img_tr[0].std([1,2])\n",
    "print(\"mean and std before normalize:\")\n",
    "print(\"Mean of the image:\", mean)\n",
    "print(\"Std of the image:\", std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(256),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean,std),\n",
    "    ])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(256),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean,std),\n",
    "    ])\n",
    "\n",
    "class ArtDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self,images,labels,transform=None,):\n",
    "        self.data = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        img_pil = Image.open(self.data[index])\n",
    "        img_pil = img_pil.convert('RGB')\n",
    "        if self.transform is not None:\n",
    "            img_pil = self.transform(img_pil)\n",
    "        label = torch.tensor(self.labels[index])\n",
    "        return(img_pil,label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author Train and Eval data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train into train and val for the author lable\n",
    "X_train_auth, X_val_auth, y_train_auth, y_val_auth = train_test_split(data['image'],data['author'], test_size=0.2, random_state=42)\n",
    "train_dataset_auth = ArtDataset(X_train_auth.values,y_train_auth.values,transform=train_transform)\n",
    "val_dataset_auth = ArtDataset(X_val_auth.values,y_val_auth.values,transform=test_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Style Train and Eval data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train into train and val for the author lable\n",
    "X_train_style, X_val_style, y_train_style, y_val_style = train_test_split(data['image'],data['style'], test_size=0.2, random_state=42)\n",
    "train_dataset_style = ArtDataset(X_train_style.values,y_train_style.values,transform=train_transform)\n",
    "val_dataset_style = ArtDataset(X_val_style.values,y_val_style.values,transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = train_dataset_auth.data[0]\n",
    "author = train_dataset_auth.labels[0]\n",
    "sty = train_dataset_style.labels[0]\n",
    "display(Image.open(image))\n",
    "print(f\"Author: {id2label_auth[author]}\")\n",
    "print(f\"Style: {id2label_sty[sty]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train_auth, X_val_auth, y_train_auth, y_val_auth, X_train_style, X_val_style, y_train_style, y_val_style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model\n",
    "\n",
    "Regarding the model, we decided to go with a Convolutional Neural Network (CNN) augmented with visual attention mechanisms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AttentionBlock\n",
    "\n",
    "To enhance the model's ability to focus on relevant regions within the input images, we designed and implemented the AttentionBlock. This component enables the network to dynamically adjust the importance of local features based on their relevance to the task at hand. By incorporating attention mechanisms, we aimed to improve the model's discriminative power and performance, particularly in scenarios where selective feature attention is beneficial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, in_features_l, in_features_g, attn_features, up_factor, normalize_attn=True):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.up_factor = up_factor\n",
    "        self.normalize_attn = normalize_attn\n",
    "        self.W_l = nn.Conv2d(in_channels=in_features_l, out_channels=attn_features, kernel_size=3, padding=1, bias=False)\n",
    "        self.W_g = nn.Conv2d(in_channels=in_features_g, out_channels=attn_features, kernel_size=3, padding=1, bias=False)\n",
    "        self.phi = nn.Conv2d(in_channels=attn_features, out_channels=1, kernel_size=1, padding=0, bias=True)\n",
    "        \n",
    "    def forward(self, l, g):\n",
    "        N, C, W, H = l.size()\n",
    "        l_ = self.W_l(l)\n",
    "        g_ = self.W_g(g)\n",
    "        g_ = F.interpolate(g_, size=(W, H), mode='bilinear', align_corners=False)\n",
    "        c = self.phi(F.relu(l_ + g_)) # batch_sizex1xWxH\n",
    "        \n",
    "        # compute attn map\n",
    "        if self.normalize_attn:\n",
    "            a = F.softmax(c.view(N,1,-1), dim=2).view(N,1,W,H)\n",
    "        else:\n",
    "            a = torch.sigmoid(c)\n",
    "        # re-weight the local feature\n",
    "        f = torch.mul(a.expand_as(l), l) # batch_sizexCxWxH\n",
    "        if self.normalize_attn:\n",
    "            output = f.view(N,C,-1).sum(dim=2) # weighted sum\n",
    "        else:\n",
    "            output = F.adaptive_avg_pool2d(f, (1,1)).view(N,C) # global average pooling\n",
    "        return a, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer Learning with VGG16\n",
    "\n",
    "Initially, we attempted to implement the VGG network architecture from scratch. However, due to the computational complexity and time constraints associated with training such a deep network, we opted for transfer learning. Specifically, we leveraged the pre-trained VGG16 model as our base network.\n",
    "\n",
    "By utilizing transfer learning, we could benefit from the features learned by VGG16 on a large dataset (e.g., ImageNet) and focus our efforts on fine-tuning the model for our specific task. This approach significantly reduced the training time and computational resources required while still enabling us to achieve satisfactory performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #VGG\n",
    "# class VGG(nn.Module):\n",
    "#     def __init__(self, num_classes):\n",
    "#         super(VGG,self).__init__()\n",
    "#         self.layer1 = nn.Sequential(\n",
    "#             nn.Conv2d(3,64,kernel_size=3,stride=1,padding=1),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU(),\n",
    "#         )\n",
    "#         self.layer2 = nn.Sequential(\n",
    "#             nn.Conv2d(64,64,kernel_size=3,stride=1,padding=1),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(2,2)\n",
    "#         )\n",
    "#         self.layer3 = nn.Sequential(\n",
    "#             nn.Conv2d(64,128,kernel_size=3,stride=1,padding=1),\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.ReLU(),\n",
    "#         )\n",
    "#         self.layer4 = nn.Sequential(\n",
    "#             nn.Conv2d(128,128,kernel_size=3,stride=1,padding=1),\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(2,2)\n",
    "#         )\n",
    "#         self.layer5 = nn.Sequential(\n",
    "#             nn.Conv2d(128,256,kernel_size=3,stride=1,padding=1),\n",
    "#             nn.BatchNorm2d(256),\n",
    "#             nn.ReLU(),\n",
    "#         )\n",
    "#         self.layer6 = nn.Sequential(\n",
    "#             nn.Conv2d(256,256,kernel_size=3,stride=1,padding=1),\n",
    "#             nn.BatchNorm2d(256),\n",
    "#             nn.ReLU(),\n",
    "#         )\n",
    "#         self.layer7 = nn.Sequential(\n",
    "#             nn.Conv2d(256,256,kernel_size=3,stride=1,padding=1),\n",
    "#             nn.BatchNorm2d(256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(2,2)\n",
    "#         )\n",
    "#         self.layer8 = nn.Sequential(\n",
    "#             nn.Conv2d(256,512,kernel_size=3,stride=1,padding=1),\n",
    "#             nn.BatchNorm2d(512),\n",
    "#             nn.ReLU(),\n",
    "#         )\n",
    "#         self.layer9 = nn.Sequential(\n",
    "#             nn.Conv2d(512,512,kernel_size=3,stride=1,padding=1),\n",
    "#             nn.BatchNorm2d(512),\n",
    "#             nn.ReLU(),\n",
    "#         )\n",
    "#         self.layer10 = nn.Sequential(\n",
    "#             nn.Conv2d(512,512,kernel_size=3,stride=1,padding=1),\n",
    "#             nn.BatchNorm2d(512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(2,2)\n",
    "#         )\n",
    "#         self.layer11 = nn.Sequential(\n",
    "#             nn.Conv2d(512,512,kernel_size=3,stride=1,padding=1),\n",
    "#             nn.BatchNorm2d(512),\n",
    "#             nn.ReLU(),\n",
    "#         )\n",
    "#         self.layer12 = nn.Sequential(\n",
    "#             nn.Conv2d(512,512,kernel_size=3,stride=1,padding=1),\n",
    "#             nn.BatchNorm2d(512),\n",
    "#             nn.ReLU(),\n",
    "#         )\n",
    "#         self.layer13 = nn.Sequential(\n",
    "#             nn.Conv2d(512,512,kernel_size=3,stride=1,padding=1),\n",
    "#             nn.BatchNorm2d(512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(2,2)\n",
    "#         )\n",
    "#         # self.fc = nn.Sequential(\n",
    "#         #     nn.Dropout(0.5),\n",
    "#         #     nn.Linear(7*7*512,4096),\n",
    "#         #     nn.ReLU(),\n",
    "#         # )\n",
    "#         # self.fc1 = nn.Sequential(\n",
    "#         #     nn.Dropout(0.5),\n",
    "#         #     nn.Linear(4,4096),\n",
    "#         #     nn.ReLU(),\n",
    "#         # )\n",
    "#         # self.fc2 = nn.Sequential(\n",
    "#         #     nn.Linear (4096, num_classes))\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.layer1(x)\n",
    "#         x = self.layer2(x)\n",
    "#         x = self.layer3(x)\n",
    "#         x = self.layer4(x)\n",
    "#         x = self.layer5(x)\n",
    "#         x = self.layer6(x)\n",
    "#         l = self.layer7(x)\n",
    "#         x = self.layer8(l)\n",
    "#         x = self.layer9(x)\n",
    "#         x = self.layer10(x)\n",
    "#         x = self.layer11(x)\n",
    "#         x = self.layer12(x)\n",
    "#         g = self.layer13(x)\n",
    "#         x = x.reshape(x.size(0),-1)\n",
    "#         # x = self.fc(x)\n",
    "#         # x = self.fc1(x)\n",
    "#         # x = self.fc2(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction of Our Model\n",
    "\n",
    "Building upon the VGG16 base, we constructed our network by adding additional layers and incorporating attention mechanisms. This allowed us to tailor the model to our particular classification task while capitalizing on the robust feature extraction capabilities of VGG16.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benefits of Transfer Learning and Attention Mechanisms\n",
    "\n",
    "Utilizing transfer learning with VGG16 and integrating attention mechanisms provided several advantages:\n",
    "- **Time Efficiency:** Transfer learning expedited the model development process by leveraging pre-trained weights, reducing the need for extensive training on our dataset.\n",
    "- **Performance Enhancement:** Attention mechanisms allowed the model to focus on salient features, potentially improving classification accuracy and robustness.\n",
    "- **Resource Conservation:** By reusing pre-trained weights and incorporating attention mechanisms, we optimized resource utilization, making the model more practical for deployment in resource-constrained environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArtCNN(nn.Module):\n",
    "    def __init__(self, num_classes, normalize_attn=False, dropout=None):\n",
    "        super(ArtCNN, self).__init__()\n",
    "        net = torchvision.models.vgg19_bn(weights=torchvision.models.VGG19_BN_Weights.DEFAULT)\n",
    "        self.conv_block1 = nn.Sequential(*list(net.features.children())[0:6])\n",
    "        self.conv_block2 = nn.Sequential(*list(net.features.children())[7:13])\n",
    "        self.conv_block3 = nn.Sequential(*list(net.features.children())[14:23])\n",
    "        self.conv_block4 = nn.Sequential(*list(net.features.children())[24:33])\n",
    "        self.conv_block5 = nn.Sequential(*list(net.features.children())[34:43])\n",
    "        self.pool = nn.AdaptiveAvgPool2d(7)\n",
    "        self.dpt = None\n",
    "        if dropout is not None:\n",
    "            self.dpt = nn.Dropout(dropout)\n",
    "        self.cls = nn.Linear(in_features=25856, out_features=num_classes, bias=True)\n",
    "        \n",
    "       # initialize the attention blocks defined above\n",
    "        self.attn1 = AttentionBlock(256, 512, 256, 8, normalize_attn=normalize_attn)\n",
    "        self.attn2 = AttentionBlock(512, 512, 256, 4, normalize_attn=normalize_attn)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        block1 = self.conv_block1(x)       # /1\n",
    "        pool1 = F.max_pool2d(block1, 2, 2) # /2\n",
    "        block2 = self.conv_block2(pool1)   # /2\n",
    "        pool2 = F.max_pool2d(block2, 2, 2) # /4\n",
    "        block3 = self.conv_block3(pool2)   # /4\n",
    "        pool3 = F.max_pool2d(block3, 2, 2) # /8\n",
    "        block4 = self.conv_block4(pool3)   # /8\n",
    "        pool4 = F.max_pool2d(block4, 2, 2) # /16\n",
    "        block5 = self.conv_block5(pool4)   # /16\n",
    "        pool5 = F.max_pool2d(block5, 2, 2) # /32\n",
    "        N, __, __, __ = pool5.size()\n",
    "        g = self.pool(pool5).view(N,-1)\n",
    "        a1, g1 = self.attn1(pool3, pool5)\n",
    "        a2, g2 = self.attn2(pool4, pool5)\n",
    "        g_hat = torch.cat((g,g1,g2), dim=1) # batch_size x C\n",
    "        if self.dpt is not None:\n",
    "            g_hat = self.dpt(g_hat)\n",
    "        out = self.cls(g_hat)\n",
    "\n",
    "        return [out, a1, a2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, optimizer, criterion, train_loader, val_loader, epochs=10):\n",
    "    \n",
    "    with tqdm(total=epochs) as pbar:\n",
    "        for _ in range(epochs):\n",
    "            # Training phase\n",
    "            model.train()\n",
    "            for _, data in enumerate(train_loader, 0):\n",
    "                inputs, labels = data[0].to(device), data[1].to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs, _, _ = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            pbar.update(1)\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                images, labels = data[0].to(device), data[1].to(device)\n",
    "                # calculate outputs by running images through the network\n",
    "                outputs,_,_ = model(images)\n",
    "                loss = criterion(outputs,labels)\n",
    "                # the class with the highest energy is what we choose as prediction\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct // total\n",
    "    print(f'Accuracy: {accuracy}%')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(val_loader, model, criterion, device):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs,_,_ = model(images)\n",
    "            loss = criterion(outputs,labels)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct // total\n",
    "    print(f'Accuracy: {accuracy}%')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "## Training Process\n",
    "1. **Load and Preprocess Data**: Prepare the dataset by loading and applying necessary preprocessing steps.\n",
    "2. **Define Model Architecture**: Set up the visual Transformer model, including the encoder and decoder components.\n",
    "3. **Set Loss Function and Optimizer**: Choose appropriate methods for training the model.\n",
    "4. **Training Loop**: Iterate through the dataset, updating model parameters to minimize the loss function.\n",
    "5. **Validation**: Evaluate the model on a separate validation dataset to monitor performance and avoid overfitting.\n",
    "6. **Metrics Monitoring**: Track metrics like loss and accuracy to assess training progress.\n",
    "\n",
    "### Hyperparameters\n",
    "- **Learning Rate**: Determines the step size during gradient descent.\n",
    "- **Batch Size**: Number of samples processed before updating the model.\n",
    "- **Epochs**: Number of times the entire training dataset is passed through the model.\n",
    "- **Optimizer**: Algorithm used for updating model parameters (e.g., AdamW)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, criterion, train_loader, val_loader, epochs=10, early_stopping_patience=25):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "    \n",
    "    with tqdm(total=epochs) as pbar:\n",
    "        for epoch in range(epochs):\n",
    "            # Training phase\n",
    "            model.train()\n",
    "            train_loss = 0.0\n",
    "            for _, data in enumerate(train_loader, 0):\n",
    "                inputs, labels = data[0].to(device), data[1].to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs, _, _ = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item()\n",
    "            train_losses.append(train_loss / len(train_loader))\n",
    "\n",
    "            # Validation phase\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for data in val_loader:\n",
    "                    images, labels = data[0].to(device), data[1].to(device)\n",
    "                    outputs, _, _ = model(images)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "                val_losses.append(val_loss / len(val_loader))\n",
    "\n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                epochs_without_improvement = 0\n",
    "            else:\n",
    "                epochs_without_improvement += 1\n",
    "                if epochs_without_improvement >= early_stopping_patience:\n",
    "                    print(f'Early stopping at epoch {epoch + 1}')\n",
    "                    break\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.set_description(f'Train loss: {train_losses[-1]:.3f} | Val loss: {val_losses[-1]:.3f}')\n",
    "            pbar.update(1)\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters Search\n",
    "\n",
    "To get the best hyperparameters for each model we decided to use the Optuna Library. This library is a hyperparameter optimization framework applicable to machine learning models. It has been designed with a focus on simplicity and ease of use, while still providing a wide range of features and capabilities. Optuna is a popular choice for hyperparameter optimization due to its flexibility, scalability. It also supports a variety of use cases, including traditional machine learning models, deep learning models, and more.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_auth(trial):\n",
    "    # Define search space for hyperparameters\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1,log=True, step = 1e-5)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64])\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5, step=0.1)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-4,log=True, step =1e-6) \n",
    "    epochs = trial.suggest_int('epochs', 5, 50, step=5)\n",
    "    early_stopping_patience = trial.suggest_int('early_stopping_patience', 5, 20, step=5)\n",
    "\n",
    "    # Train the model with current hyperparameters\n",
    "    model = ArtCNN(num_classes=len(labels_author), dropout=dropout_rate).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    train_loader = DataLoader(train_dataset_auth, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset_auth, batch_size=batch_size, shuffle=False)\n",
    "    _, val_loss = train(model, optimizer, criterion, train_loader, val_loader, epochs = epochs, early_stopping_patience=early_stopping_patience)\n",
    "    accuracy = calculate_accuracy(val_loader, model, criterion, device)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Create a study object and optimize the objective function\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective_auth, n_trials=10)\n",
    "\n",
    "# Get the best hyperparameters and accuracy\n",
    "best_params_auth = study.best_params\n",
    "best_accuracy_auth = study.best_value\n",
    "#{'learning_rate': 5.1481519513568155e-05, 'batch_size': 32, 'dropout_rate': 0.25189548904234554, 'weight_decay': 3.701365753127736e-05, 'epochs': 24, 'early_stopping_patience': 17}\n",
    "print(\"Best hyperparameters Author:\", best_params_auth)\n",
    "print(\"Best accuracy Author:\", best_accuracy_auth)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_style(trial):\n",
    "    # Define search space for hyperparameters\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1,log=True, step = 1e-5)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [32, 64])\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5, step=0.1)\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-4,log=True, step =1e-6) \n",
    "    epochs = trial.suggest_int('epochs', 5, 50, step=5)\n",
    "    early_stopping_patience = trial.suggest_int('early_stopping_patience', 5, 20, step=5)\n",
    "\n",
    "    # Train the model with current hyperparameters\n",
    "    model = ArtCNN(num_classes=len(labels_sty), dropout=dropout_rate).to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    train_loader = DataLoader(train_dataset_style, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset_style, batch_size=batch_size, shuffle=False)\n",
    "    _, val_loss = train(model, optimizer, criterion, train_loader, val_loader, epochs = epochs, early_stopping_patience=early_stopping_patience)\n",
    "    accuracy = calculate_accuracy(val_loader, model, criterion, device)\n",
    "\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "# Create a study object and optimize the objective function\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective_style, n_trials=100)\n",
    "\n",
    "# Get the best hyperparameters and accuracy\n",
    "best_params_sty = study.best_params\n",
    "best_accuracy_sty = study.best_value\n",
    "\n",
    "print(\"Best hyperparameters Style:\", best_params_sty)\n",
    "print(\"Best accuracy Style:\", best_accuracy_sty)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Hyperparameters Grid Search\n",
    "# from itertools import product\n",
    " \n",
    "# learning_rates = [0.1, 0.01, 0.001,0.0001]\n",
    "# weight_decays = [0.1, 0.01, 0.001,0.0001]\n",
    "# dropouts = [0.2, 0.3,0.5]\n",
    "# batch_sizes = [32, 64]\n",
    "\n",
    "# best_accuracy = 0\n",
    "# best_learning_rate = 0\n",
    "# for lr, batch_size, dropout_rate, weight_decay in product(learning_rates, batch_sizes, dropouts, weight_decays):\n",
    "#     model = ArtCNN(num_classes=len(labels_auth), normalize_attn=True, dropout=dropout_rate)\n",
    "#     model.to(device)\n",
    "#     train_loader = DataLoader(train_dataset_auth, batch_size=batch_size, shuffle=True)\n",
    "#     val_loader = DataLoader(val_dataset_auth, batch_size=batch_size, shuffle=False)\n",
    "#     optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "#     accuracy,_,_ = train_and_evaluate(model, optimizer, nn.CrossEntropyLoss(), train_loader, val_loader)\n",
    "#     if accuracy > best_accuracy:\n",
    "#         best_accuracy = accuracy\n",
    "#         best_learning_rate = lr\n",
    "#         best_batch_size = batch_size\n",
    "#         best_dropout_rate = dropout_rate\n",
    "#         best_weight_decay = weight_decay\n",
    "# print(f'Best accuracy: {best_accuracy}, Best learning rate: {best_learning_rate}, Best batch size: {best_batch_size}, Best dropout rate: {best_dropout_rate}, Best weight decay: {best_weight_decay}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not best_params_auth:\n",
    "    best_params_auth = {'learning_rate': 5.1481519513568155e-05, 'batch_size': 32, 'dropout_rate': 0.25189548904234554, 'weight_decay': 3.701365753127736e-05, 'epochs': 24, 'early_stopping_patience': 17}\n",
    "if not best_accuracy_sty:\n",
    "    best_params_sty = {'learning_rate': 1.0225636410036674e-05, 'batch_size': 32, 'dropout_rate': 0.4978375095238207, 'weight_decay': 1.1995772075099324e-06, 'epochs': 50, 'early_stopping_patience': 12}\n",
    "    \n",
    "trainloader_auth = DataLoader(train_dataset_auth, batch_size=best_params_auth['batch_size'], shuffle=True)\n",
    "testloader_auth = DataLoader(val_dataset_auth, batch_size=best_params_auth['batch_size'], shuffle=False)\n",
    "trainloader_style = DataLoader(train_dataset_style, batch_size=best_params_sty['batch_size'], shuffle=True)\n",
    "testloader_style = DataLoader(val_dataset_style, batch_size=best_params_sty['batch_size'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = next(iter(trainloader_auth))\n",
    "print(f\"Feature batch shape: {features.size()}\")\n",
    "print(f\"Labels batch shape: {labels.size()}\")\n",
    "img = features[0].squeeze().permute(1, 2, 0)\n",
    "label = labels[0]\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "print(f\"Author: {labels_author[label]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_auth = ArtCNN(len(labels_author),dropout=best_params_auth['dropout']).to(device)\n",
    "model_style = ArtCNN(len(labels_sty),dropout=best_params_sty['dropout']).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choice of Loss Function and Optimizer\n",
    "Selecting an appropriate loss function and optimizer is critical for effective model training.\n",
    "\n",
    "1. **Cross-Entropy Loss**:\n",
    "   - **Applicability**: Suitable for classification tasks, where the goal is to categorize input images into predefined classes.\n",
    "   - **Compatibility**: Works well with models that output probabilities for different classes via a softmax layer.\n",
    "   - **Training Objective**: Minimizing this loss function helps the model improve its prediction accuracy.\n",
    "\n",
    "2. **AdamW Optimizer**:\n",
    "   - **Adaptive Learning Rate**: Adjusts learning rates for each parameter based on the magnitude of gradients, promoting efficient convergence.\n",
    "   - **Weight Decay**: Regularizes the model by penalizing large weights, reducing the risk of overfitting.\n",
    "   - **Robustness**: Performs well across various tasks and architectures.\n",
    "   - **Stability**: Ensures stable and faster convergence by maintaining separate learning rates for each parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_auth = optim.AdamW(model_auth.parameters(),lr=best_params_auth['learning_rate'], weight_decay=best_params_auth['weight_decay'])\n",
    "optimizer_style = optim.AdamW(model_style.parameters(),lr=best_params_sty['learning_rate'], weight_decay=best_params_sty['weight_decay'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training the model for author...')\n",
    "auth_train_losses, auth_test_loss = train(model_auth, optimizer_auth, criterion, trainloader_auth, testloader_auth, epochs=best_params_auth['epochs'], early_stopping_patience=best_params_auth['early_stopping_patience'])\n",
    "print('Training the model for style...')\n",
    "style_train_losses, style_test_loss = train(model_style, optimizer_style, criterion, trainloader_style, testloader_style, epochs=best_params_sty['epochs'],early_stopping_patience=best_params_sty['early_stopping_patience'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_loss(loss_values, title=\"Loss Plot\"):\n",
    "    \"\"\"\n",
    "    Plot the loss values over training epochs.\n",
    "\n",
    "    Parameters:\n",
    "    - loss_values: A list or array of loss values for each epoch.\n",
    "    - title: The title of the plot (optional).\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(loss_values[0], marker='o', linestyle='-', label='train')\n",
    "    plt.plot(loss_values[1], marker='o', linestyle='-',label='val')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss([auth_train_losses,auth_test_loss], \"Author Training Loss\")\n",
    "plot_loss([style_train_losses,style_test_loss], \"Style Training Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter_auth = iter(testloader_auth)\n",
    "dataiter_style = iter(testloader_style)\n",
    "\n",
    "images, label_auth = next(dataiter_auth)\n",
    "images, label_sty = next(dataiter_style)\n",
    "\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth Author: ', '; '.join('%5s' % labels_author[label_auth[j]] for j in range(4)))\n",
    "print('GroundTruth Style: ', '; '.join('%5s' % labels_sty[label_sty[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_auth.eval()\n",
    "images = images.to(device)\n",
    "\n",
    "outputs = model_auth(images)[0].to(device)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "print('Predicted: ', '; '.join('%5s' % labels_author[predicted[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_style.eval()\n",
    "images = images.to(device)\n",
    "outputs = model_auth(images)[0].to(device)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "print('Predicted: ', '; '.join('%5s' % labels_sty[predicted[j]] for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_accuracy(testloader_auth, model_auth, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_accuracy(testloader_style, model_style, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
    "torch.save(model_auth.state_dict(), './model_checkpoints/model_auth.pth')\n",
    "torch.save(model_style.state_dict(), './model_checkpoints/model_style.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m111"
  },
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

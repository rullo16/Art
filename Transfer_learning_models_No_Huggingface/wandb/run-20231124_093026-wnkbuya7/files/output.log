No such comm: 9709c5c7e9cb4003b3393104cb48f2b4
No such comm: e812c9a052a34029a56ad8b44ddf4d07
No such comm: 3067bc6731664d94a1e9a14952e412d9
No such comm: ba21f7d5efef4dacbca8423c52da71d7
No such comm: 650cb093f50441b8a066c7de6ec18eaa
No such comm: a5636d42091c433883b3558914a5f6d2
No such comm: a0332183f251447ea1baa434118d0520
No such comm: 9ee71cbb0cb147e2b52f2e5f9985948b
No such comm: 4c2f047b8fb04fc1a5bcff031f34a3ac
No such comm: 2be856d18849446f823f9495fd4ae1e1
No such comm: 55fc2124e8f1435c9fa501c853ea20e4
No such comm: 799ba2f40124427ea86c14f86d7b98d0
No such comm: 0c52aaf6fd6f4eb3a30e1bde73e5cc26
No such comm: 442a3b2d5b43406ca140418ec0de07df
No such comm: 4245274826dc4d3ba94a5fb2eaaffca9
No such comm: 33deab974a854ca3ab433830274a530a
  5%|▍         | 147/3000 [00:01<00:30, 95.02it/s]
















 99%|█████████▊| 2960/3000 [00:33<00:00, 84.18it/s]
mean and std before normalize:
Mean of the image: tensor([0.4685, 0.3801, 0.3472])
Std of the image: tensor([0.2129, 0.1872, 0.1589])
Feature batch shape: torch.Size([16, 3, 256, 256])
Labels batch shape: 16
Label: tensor([[13925, 45749, 59396, 26071, 41367, 44606, 51753, 26071, 15885, 41312,
         54744, 26071, 25325, 15914, 17054, 13925, 13632, 21639, 41683, 46902,
         47331, 28636, 48636, 24765, 17308, 55772,  7916, 45749, 55772, 63285,
         34661, 55772, 15224, 32344, 26071, 14784, 25132, 26071, 60207, 37305,
         26071, 19511, 25132, 42796, 12448, 19690, 24731,  1149, 55772, 52560],
        [13428, 58722, 59396, 26967, 26071, 24731,  1149, 38316,  2380, 19690,
         61502, 17054, 31621, 17054, 44550, 44550, 44550, 44550, 44550, 44550,
         44550, 44550, 44550, 44550, 44550, 44550, 44550, 44550, 44550, 44550,
         44550, 44550, 44550, 44550, 44550, 44550, 44550, 44550, 44550, 44550,
         44550, 44550, 44550, 44550, 44550, 44550, 44550, 44550, 44550, 44550],
        [13925, 19279, 59396,  5274,  2990, 53790, 17054,   948, 17054, 44550,
         44550, 44550, 44550, 44550, 44550, 44550, 44550, 44550, 44550, 44550,
         44550, 44550, 44550, 44550, 44550, 44550, 44550, 44550, 44550, 44550,
         44550, 44550, 44550, 44550, 44550, 44550, 44550, 44550, 44550, 44550,
         44550, 44550, 44550, 44550, 44550, 44550, 44550, 44550, 44550, 44550],
        [13428,  6971, 49373,  7916,  6148, 38616, 52907, 40514, 34661,  9091,
         13665, 14460, 46902, 57092, 17054, 35696, 59396, 25126, 37305, 26071,
         64042, 30936, 19690, 27339, 36989, 14959, 53956, 36502, 61790, 51483,
         49654, 55772,  9091, 53891, 19690, 52907, 19238, 41805, 43800, 30250,
         19690, 34661, 14066, 19690, 54744, 56685, 19690,  7916,  6148, 37920],
        [13428, 44606, 59396,   209, 25132, 46902, 15046, 25132, 43987, 13649,
         25132, 26071,  1994, 38704, 17054, 41657, 33073, 46902, 24931, 47577,
         25132, 23938, 12445, 52907, 26071, 52196, 38704, 60635, 51753, 46902,
         15046, 25132, 33683, 19690, 60408, 14460, 46902, 38616, 62334, 19690,
         32859, 52907,  5826,  5777, 27200, 55772, 36253, 12544,  3785, 26071],
        [19025, 21901, 26071, 64789, 59396, 36744, 51753, 26071, 34606, 15754,
         25132, 26071,  1070, 35962, 19690, 41907, 51753, 26071, 55198, 15754,
         19025, 21901, 26071,  8282, 43675, 43684, 20910, 17054, 41657, 41644,
          5777,    45, 34072, 25132, 46902, 44653, 36972, 55772, 37354, 52583,
         26071,  6531, 37305, 46902, 38068, 54979,  3785, 64042, 22203,  6803],
        [56899, 26071, 16796, 25132, 26071, 40665, 21084, 25163, 46902, 36862,
         18595, 37305, 58815, 34661, 44474, 31488, 17054, 60181,  9568, 62977,
         12544, 54997, 55772,  7916, 63484,  1590, 55772,  9929, 46891, 44606,
         17054, 61014, 26071, 64919, 25132, 38711, 11811, 21084, 16344, 30308,
         41876, 36101, 37540, 16344, 10518, 60456, 14460, 26071, 26613, 30160],
        [48320, 23737, 53344, 16764, 62597,  3785, 26071,  9922, 25132, 63658,
         55772, 26071, 44592, 25132, 32298, 17054, 13925, 19844, 39735, 24498,
         14589, 37305, 39664,  4733,   414,  4684, 45046, 19690, 35614, 25132,
         26071, 47357, 19690,  4490, 27579,  7504, 15783, 14460, 26071,  4733,
         10631, 34661, 54171, 46902, 28934, 19690, 26071, 22891, 13407, 25132],
        [41782, 39735, 46902, 26357, 25132, 57839, 19690, 65513, 39735, 56047,
          1989,  6789, 14460, 46902, 12102, 25132, 24470, 62852, 19690,  7440,
         32481, 10155, 19690, 50610, 43159,  4390, 37305, 45329, 12544, 46902,
         44911, 25132, 53641,  3785, 15219,  1471, 17054, 33820, 19690, 46902,
         64384, 38621, 52907, 41918, 28248, 46172, 54744, 46902, 11118, 19690],
        [48958, 28387, 34661, 43047, 47237, 19690, 18081, 38335, 16679, 37305,
         45329, 12544, 29248, 37305, 29691, 48323, 55772,  4756,  5744, 34661,
          3682, 17054, 25539, 31748, 62977, 60635, 37305,  3785, 26071,  4417,
         23344, 33253, 25132, 26071,  4270, 37305, 31549,   209, 25132, 42796,
         42931, 14959, 62090, 25132, 46902, 25699,  2383, 17054, 36101,  7916],
        [13428, 32053, 15681, 52907, 12242, 10585, 47940, 37305, 46902, 11498,
         36972, 24737, 12544, 12445, 52907, 28592, 34661, 48323, 34661,  9091,
         59176, 36253, 17054, 63532,   194, 19690, 28636, 44024, 62977, 46902,
         11768, 27630, 51753, 26071, 10428, 28592, 64083, 55581, 52560, 54744,
         46902, 10428, 38621, 55772, 26071, 30603, 25132, 38484, 42796, 32071],
        [48320, 13682, 23157, 15911,  5137, 26071, 27936,  3785, 55953, 25132,
         26071, 48849, 26071, 41367, 18036, 55772, 26071, 43129, 25132, 26071,
         54109, 42429, 15250, 17054, 13925, 36121, 25132, 26071, 43129, 19690,
         36792, 19690, 24765, 48776, 26071, 14959, 52907, 49228, 25132, 26071,
         41464, 34661, 26071, 10390, 16900, 25132, 26071, 15885, 17054, 13925],
        [13428, 59396,  7916, 50290, 37305,  3424, 14959, 15525,  1710, 17054,
         44550, 44550, 44550, 44550, 44550, 44550, 44550, 44550, 44550, 44550,
         44550, 44550, 44550, 44550, 44550, 44550, 44550, 44550, 44550, 44550,
         44550, 44550, 44550, 44550, 44550, 44550, 44550, 44550, 44550, 44550,
         44550, 44550, 44550, 44550, 44550, 44550, 44550, 44550, 44550, 44550],
        [13925,  4639, 25132, 26071, 11961, 59396, 27945, 62745, 55198, 51753,
         26071, 15623, 48025, 25132, 26071,  9612, 22203, 14706, 42796,  5777,
         21297, 19690, 55772, 26071, 44805, 25132, 65363, 37305, 36968, 19690,
          1257, 52907, 15219, 22896, 25132, 51375, 60554,  9091, 17188, 62425,
         34661,  9507, 55185, 19690, 65513, 59396, 19238, 36267, 25132, 61577],
        [53344, 54596, 26071, 47331,  3785, 26071, 33870, 25132, 26071, 43129,
         25132, 41694, 15246, 39199, 55772, 52560, 17054, 13925,  5627, 27893,
         12501, 59396, 26071,  2441, 52549, 43129, 33870, 51753, 46902, 27382,
         52347, 37305, 43684, 36390, 23802, 26071,  7533, 17054, 48320, 32660,
         10240, 47331, 53344, 60652, 26071, 42979,  1165, 37193, 33870, 25132],
        [48320, 15219,  1471, 25132, 65174, 32960, 19690, 26071, 33253, 24737,
         22728,  7916, 37621, 41845, 34210, 26967, 46902, 24908, 56882, 58994,
         26071, 46854, 17054, 13428, 13222,  5415, 45329, 12544, 64469, 14460,
         26071, 22622, 25132, 21375, 63025, 19690, 65513, 16344,  6826, 42216,
100%|██████████| 3000/3000 [00:34<00:00, 87.09it/s]
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
c:\Users\Fede\anaconda3\envs\tf\lib\site-packages\torchvision\models\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
c:\Users\Fede\anaconda3\envs\tf\lib\site-packages\torchvision\models\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)





















































































































































Epoch: 1, Batch: 150, Loss: 8.706202507019043: 100%|██████████| 150/150 [10:38<00:00,  4.26s/it]













































































































































Epoch: 2, Batch: 150, Loss: 4.573533535003662: 100%|██████████| 150/150 [10:41<00:00,  4.28s/it]
Batch: 0, Loss: 5.680264949798584
Batch: 1, Loss: 4.387805938720703
Batch: 2, Loss: 7.0612592697143555
Batch: 3, Loss: 7.746406078338623
Batch: 4, Loss: 7.819716930389404
Batch: 5, Loss: 7.722900390625
Batch: 6, Loss: 8.733362197875977
Batch: 7, Loss: 8.23172664642334
Batch: 8, Loss: 7.858531475067139
Batch: 9, Loss: 8.051840782165527
Batch: 10, Loss: 8.073996543884277
Batch: 11, Loss: 8.440295219421387
Batch: 12, Loss: 8.546507835388184
Batch: 13, Loss: 2.445807456970215
Batch: 14, Loss: 9.07966136932373
Batch: 15, Loss: 8.226590156555176
Batch: 16, Loss: 2.607015371322632
Batch: 17, Loss: 8.023365020751953
Batch: 18, Loss: 5.7737226486206055
Batch: 19, Loss: 8.613298416137695
Batch: 20, Loss: 4.655235290527344
Batch: 21, Loss: 8.282466888427734
Batch: 22, Loss: 6.749423980712891
Batch: 23, Loss: 3.396946907043457
Batch: 24, Loss: 5.434724807739258
Batch: 25, Loss: 7.833747386932373
Batch: 26, Loss: 3.185206651687622
Batch: 27, Loss: 2.4628920555114746
Batch: 28, Loss: 8.707962989807129
Batch: 29, Loss: 7.723543167114258
Batch: 30, Loss: 8.823680877685547
Batch: 31, Loss: 8.137455940246582
Batch: 32, Loss: 3.41688871383667
Batch: 33, Loss: 8.113775253295898
Batch: 34, Loss: 9.168655395507812
Batch: 35, Loss: 9.286918640136719
Batch: 36, Loss: 7.698408126831055
Batch: 37, Loss: 7.004498481750488
Batch: 0, Loss: 5.81649112701416
Batch: 1, Loss: 4.208978652954102
Batch: 2, Loss: 7.266912460327148
Batch: 3, Loss: 7.725290298461914
Batch: 4, Loss: 7.794924736022949
Batch: 5, Loss: 7.944828510284424
Batch: 6, Loss: 8.602620124816895
Batch: 7, Loss: 8.066679954528809
Batch: 8, Loss: 7.942596912384033
Batch: 9, Loss: 7.967313766479492
Batch: 10, Loss: 7.959738731384277
Batch: 11, Loss: 8.360723495483398
Batch: 12, Loss: 8.444928169250488
Batch: 13, Loss: 2.376065969467163
Batch: 14, Loss: 9.011327743530273
Batch: 15, Loss: 8.171089172363281
Batch: 16, Loss: 2.5602951049804688
Batch: 17, Loss: 7.955429553985596
Batch: 18, Loss: 5.811910629272461
Batch: 19, Loss: 8.573456764221191
Batch: 20, Loss: 4.737773418426514
Batch: 21, Loss: 8.335675239562988
Batch: 22, Loss: 6.833962917327881
Batch: 23, Loss: 3.3427047729492188
Batch: 24, Loss: 5.268642425537109
Batch: 25, Loss: 7.790713310241699
Batch: 26, Loss: 3.150843858718872
Batch: 27, Loss: 2.516328811645508
Batch: 28, Loss: 8.701569557189941
Batch: 29, Loss: 7.627835273742676
Batch: 30, Loss: 8.852282524108887
Batch: 31, Loss: 8.248564720153809
Batch: 32, Loss: 3.4761667251586914
Batch: 33, Loss: 8.082687377929688
Batch: 34, Loss: 9.135455131530762
Batch: 35, Loss: 9.245339393615723
Batch: 36, Loss: 7.645381927490234
Batch: 37, Loss: 7.257260322570801
Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).
[nltk_data] Downloading package punkt to
[nltk_data]     C:\Users\Fede\AppData\Roaming\nltk_data...
[nltk_data]   Package punkt is already up-to-date!
[34m[1mwandb[39m[22m: [33mWARNING[39m Calling wandb.login() after wandb.init() has no effect.